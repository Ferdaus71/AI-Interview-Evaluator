# -*- coding: utf-8 -*-
"""AI_Interview_Evaluator.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1CAEX4GDPFDGG0YtujmX22DT6KsVStWsa

Install Dependencies
"""

!pip install -q openai-whisper transformers torch gradio librosa soundfile matplotlib reportlab plotly kaleido
!apt-get -y install ffmpeg

"""Import Libraries"""

import whisper
import torch
import librosa
import numpy as np
import matplotlib.pyplot as plt
import gradio as gr
from transformers import pipeline
from reportlab.lib.pagesizes import letter
from reportlab.pdfgen import canvas
from reportlab.lib.utils import ImageReader
from datetime import datetime

"""Load Models"""

# Whisper model for transcription
whisper_model = whisper.load_model("small")

# Transformers pipelines
sentiment_analyzer = pipeline(
    "sentiment-analysis",
    model="distilbert-base-uncased-finetuned-sst-2-english"
)

emotion_model = pipeline(
    "text-classification",
    model="j-hartmann/emotion-english-distilroberta-base",
    return_all_scores=False
)

"""Audio Feature Extraction"""

def analyze_audio_features(path):
    y, sr = librosa.load(path, sr=None)
    rms = np.mean(librosa.feature.rms(y=y))
    zcr = np.mean(librosa.feature.zero_crossing_rate(y))
    pitches, mags = librosa.piptrack(y=y, sr=sr)
    pitch_vals = pitches[mags > np.median(mags)]
    avg_pitch = np.mean(pitch_vals) if len(pitch_vals) else 0
    return {
        "Energy": min(100, round(rms*5000,2)),
        "Pitch": min(100, round(avg_pitch/5,2)),
        "Speech Rate": min(100, round(zcr*300,2))
    }

"""Text Analysis"""

def transcribe_audio(path):
    return whisper_model.transcribe(path)["text"]

def analyze_text(text):
    sentiment = sentiment_analyzer(text)[0]
    emotion = emotion_model(text)[0]
    words = text.split()
    fillers = ['um','uh','like','you know','basically','actually']
    fcount = sum(w.lower() in fillers for w in words)
    clarity = max(0, 100 - fcount*10)
    confidence = max(0, min(100, sentiment['score']*100 - fcount*5))
    key_strength = min(100, len({w for w in words if len(w)>6})*5)
    return {
        "Tone": sentiment["label"], "Tone Score": round(sentiment["score"]*100,2),
        "Emotion": emotion["label"], "Clarity": clarity,
        "Confidence": confidence, "Keyword Strength": key_strength,
        "Transcription": text
    }

"""Generate Tips"""

def generate_tips(text_res, audio_res, overall_score):
    tips = []

    if text_res["Tone"] in ["NEGATIVE","LABEL_1"]:
        tips.append("âš ï¸ Tone seems negative; use a confident, friendly tone.")
    else:
        tips.append("âœ… Tone is positive and professional.")

    if text_res["Emotion"] in ["anger","fear","sadness"]:
        tips.append(f"ğŸ’¡ Emotion detected: {text_res['Emotion']}. Keep answers calm.")
    else:
        tips.append(f"ğŸ’¡ Emotion detected: {text_res['Emotion']}. Good emotional control!")

    if text_res["Clarity"]<70:
        tips.append("ğŸ¯ Reduce filler words like 'um', 'uh', 'actually' to improve clarity.")

    if text_res["Confidence"]<60:
        tips.append("ğŸ’ª Practice speaking confidently; maintain steady voice.")

    if text_res["Keyword Strength"]<50:
        tips.append("ğŸ”‘ Use more technical/action/role-specific keywords.")

    if audio_res["Energy"]<60:
        tips.append("âš¡ Increase vocal energy; sound enthusiastic.")
    if audio_res["Pitch"]<50 or audio_res["Pitch"]>80:
        tips.append("ğŸµ Maintain moderate pitch; avoid monotone or high pitch.")
    if audio_res["Speech Rate"]<40:
        tips.append("â±ï¸ Speak slightly faster for engagement.")
    elif audio_res["Speech Rate"]>70:
        tips.append("â±ï¸ Slow down to ensure clarity.")

    if overall_score>85:
        tips.append("ğŸŒŸ Excellent! Keep practicing to maintain strong interview skills.")
    elif overall_score>70:
        tips.append("ğŸ‘ Good job! Slight improvements can make you outstanding.")
    elif overall_score>50:
        tips.append("ğŸ§© Average. Focus on clarity, confidence, and energy.")
    else:
        tips.append("âš™ï¸ Needs improvement. Practice aloud, focus on clarity and confidence.")

    return "\n".join(tips)

"""Generate Performance Chart"""

def generate_chart(metrics, chart_path="/content/performance_chart.png"):
    plt.figure(figsize=(8,4))
    names = list(metrics.keys())
    values = list(metrics.values())
    bars = plt.barh(names, values, color="#4CAF50")

    for bar in bars:
        plt.text(bar.get_width() + 1, bar.get_y() + bar.get_height()/2,
                 f'{bar.get_width():.1f}%', va='center')

    plt.xlim(0, 100)
    plt.xlabel("Score (%)")
    plt.title("Interview Metrics Performance")
    plt.tight_layout()
    plt.savefig(chart_path)
    plt.close()
    return chart_path

"""Generate PDF Report"""

def generate_pdf(text_res, metrics, tips, pdf_path="/content/AI_Interview_Report.pdf"):
    c = canvas.Canvas(pdf_path, pagesize=letter)
    w,h = letter

    c.setFont("Helvetica-Bold",16)
    c.drawString(150,h-60,"AI Interview Evaluation Report")
    c.setFont("Helvetica",10)
    c.drawString(150,h-80,f"Generated on: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")

    # Transcription
    y = h-140
    c.setFont("Helvetica",10)
    c.drawString(60,y,"Transcribed Text:")
    y -= 14
    for line in text_res.splitlines():
        c.drawString(60,y,line)
        y -= 14

    # Metrics
    y -= 10
    c.drawString(60,y,"Metrics:")
    y -= 14
    for k,v in metrics.items():
        c.drawString(60,y,f"{k}: {v:.1f}%")
        y -= 14

    # Tips
    y -= 10
    c.drawString(60,y,"Tips:")
    y -= 14
    for line in tips.splitlines():
        c.drawString(60,y,line)
        y -= 14
        if y < 100:
            c.showPage()
            y = h-60

    # Signature
    c.setFont("Helvetica-Bold",12)
    c.drawString(60,60,"Verified by:MD. Ferdaus Hossen â€“ Junior AI/ML Engineer at Zensoft Lab")
    c.line(60,58,400,58)

    c.save()
    return pdf_path

"""Evaluate & Interactive"""

def evaluate_interview(audio):
    text_res = analyze_text(transcribe_audio(audio))
    audio_res = analyze_audio_features(audio)

    metrics = {
        "Confidence": text_res["Confidence"],
        "Clarity": text_res["Clarity"],
        "Keyword Strength": text_res["Keyword Strength"],
        "Energy": audio_res["Energy"],
        "Pitch": audio_res["Pitch"],
        "Speech Rate": audio_res["Speech Rate"]
    }

    overall_score = round(
        metrics["Confidence"]*0.25 + metrics["Clarity"]*0.2 + metrics["Keyword Strength"]*0.15 +
        metrics["Energy"]*0.15 + metrics["Pitch"]*0.1 + metrics["Speech Rate"]*0.15, 2
    )

    tips = generate_tips(text_res, audio_res, overall_score)

    # Interactive Dashboard HTML
    dashboard_html = "<h3>ğŸ“Š Interactive Metrics Dashboard</h3>"
    for k,v in metrics.items():
        dashboard_html += f"""
        <div style='margin-bottom:10px'>
            <label>{k}: {v:.1f}%</label>
            <div style='background:#ddd;border-radius:12px;width:100%;height:20px;position:relative'>
                <div style='width:{v}%;background:#4CAF50;height:100%;border-radius:12px'></div>
            </div>
        </div>
        """

    chart_path = generate_chart(metrics)
    pdf_path = generate_pdf(text_res['Transcription'], metrics, tips)

    report_md = f"""
## ğŸ™ï¸ AI Interview Evaluation
**Transcribed Text:**
{text_res['Transcription']}

### ğŸ§  Overall Score: {overall_score}/100

### ğŸ“ Personalized Tips:
{tips}

ğŸ“„ **Download PDF Report:** [Click Here]({pdf_path})
"""
    return report_md, dashboard_html, chart_path, pdf_path

"""Dashboard"""

import gradio as gr

with gr.Blocks(css="""
    body {font-family: 'Arial', sans-serif;}
    .eval-btn {background-color:#4CAF50;color:white;font-weight:bold;padding:10px 20px;border-radius:12px;border:none;transition:0.3s;}
    .eval-btn:hover {background-color:#45a049; transform: scale(1.05);}
""") as ai_dashboard:

    gr.Markdown("<h1 style='text-align:center;color:#4CAF50'>ğŸ¤– AI Interview Evaluator</h1>")
    gr.Markdown("<p style='text-align:center;color:#555'>Interactive metrics dashboard with PDF report & performance chart</p>")

    audio_input = gr.Audio(sources=["microphone","upload"], type="filepath", label="ğŸ¤ Record/Upload Interview Answer")

    with gr.Row():
        output_text = gr.Markdown(label="ğŸ“„ Feedback & Tips")
        output_dashboard = gr.HTML(label="ğŸ“Š Interactive Dashboard")
    output_chart = gr.Image(label="ğŸ“ˆ Performance Chart")
    pdf_output = gr.File(label="ğŸ“„ Download PDF Report")

    eval_btn = gr.Button("ğŸ” Evaluate Interview", elem_classes="eval-btn")
    eval_btn.click(
        evaluate_interview,
        inputs=audio_input,
        outputs=[output_text, output_dashboard, output_chart, pdf_output]
    )

    # Footer
    gr.HTML("""
<div align="center">

**ğŸ§‘â€ğŸ’»ğŸ§‘â€ğŸ’»ğŸ§‘â€ğŸ’»ğŸ§‘â€ğŸ’» **
<h3>ğŸ§‘â€ğŸ’» Developed by :Md. Ferdaus HossenğŸ§‘â€ğŸ’»</h3>
<h5>Junior AI/ML Engineer at Zensoft Lab</h5>

<p>
  <a href="https://github.com/Ferdaus71" target="_blank" style="margin-right:10px;">
    <img src="https://cdn.jsdelivr.net/gh/simple-icons/simple-icons/icons/github.svg" width="25" height="25" alt="GitHub">
  </a>
  <a href="https://www.linkedin.com/in/ferdaus70/" target="_blank" style="margin-left:10px;">
    <img src="https://cdn.jsdelivr.net/gh/simple-icons/simple-icons/icons/linkedin.svg" width="25" height="25" alt="LinkedIn">
  </a>
</p>

</div>
""")

ai_dashboard.launch(debug=True)